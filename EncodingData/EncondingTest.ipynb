{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook test self encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from QuotesDownloader import SoftfxDownloader, QuotesType, QuotesPeriodicity\n",
    "import numpy as np\n",
    "#import tensorflow as tf \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tqdm import tqdm\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:46.737103100Z",
     "start_time": "2023-10-30T20:40:46.626160500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0-dev20230828\n",
      "C:\\Users\\Petrosyan\\.conda\\envs\\ae4\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:46.804311700Z",
     "start_time": "2023-10-30T20:40:46.719104300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plan:\n",
    "1. Download data for EURUSD\n",
    "2. Convert data to pips\n",
    "3. Convert bar prices to one pip movement. If bid was increased to X pips, X ones will be generated.\n",
    "4. Create network for encoding/decoding to dictionary.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  First step. Download M1 data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368053, 6)\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "symbols = [\"EURUSD\"]\n",
    "pips_value = [10**-5]\n",
    "N_x = 1000 #number of pips movements for NN\n",
    "\n",
    "qd = SoftfxDownloader.Downloader()\n",
    "train_raw = qd.get_quotes(symbols[0], datetime.date(2020, 1, 1), datetime.date(2021, 1, 1), QuotesPeriodicity.M1, QuotesType.Bids)\n",
    "train_raw = train_raw.to_numpy()\n",
    "print(train_raw.shape)\n",
    "print(train_raw.dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:47.502453900Z",
     "start_time": "2023-10-30T20:40:46.807312600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Second step. Convert data to pips"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "(368053,)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode input data to NN friendly format.\n",
    "bids = train_raw[:,1] / pips_value\n",
    "bids = bids.astype('f')\n",
    "bids = np.rint(bids).astype('i')\n",
    "bids.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:47.600222500Z",
     "start_time": "2023-10-30T20:40:47.502453900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Fourth step. Convert to pip steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# initializing list\n",
    "def pips2pricemovments(arr : np.ndarray) -> np.ndarray:\n",
    "    d_iter = map(lambda x, y: np.repeat(np.float16(1), y-x) if x<y else np.repeat(np.float16(0), x-y), arr[:-1], arr[1:])\n",
    "    return np.concatenate(list(d_iter))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:47.677441100Z",
     "start_time": "2023-10-30T20:40:47.600222500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file\n"
     ]
    },
    {
     "data": {
      "text/plain": "(3672304,)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_converted = \"bids_EURUSD.npy\"\n",
    "if os.path.exists(filename_converted):\n",
    "    print(\"Loading file\")\n",
    "    d = np.load(filename_converted)\n",
    "else:\n",
    "    d = pips2pricemovments(bids)\n",
    "    np.save(filename_converted, d)\n",
    "d.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:47.765727800Z",
     "start_time": "2023-10-30T20:40:47.679442100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Fifth step. NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape is (367230,).\n",
      "Test data shape is (3305074,).\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "#train/test split\n",
    "# Test your function and save all \"global\" variables within the G class (G stands for global)\n",
    "@dataclass\n",
    "class GSettings:\n",
    "    filename_converted = \"bids_EURUSD.npy\"\n",
    "    d = np.load(filename_converted)\n",
    "    N = d.shape[0]\n",
    "    train_ratio = 0.1\n",
    "    n_train = int(N * train_ratio)\n",
    "    window_size = 1024\n",
    "    batch_size = 64\n",
    "    SHUFFLE_BUFFER_SIZE = 1000\n",
    "    N_letters = 10\n",
    "\n",
    "train_data = d[:GSettings.n_train]\n",
    "test_data = d[GSettings.n_train:]\n",
    "\n",
    "print(f\"Train data shape is {train_data.shape}.\\nTest data shape is {test_data.shape}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:47.850428400Z",
     "start_time": "2023-10-30T20:40:47.764728Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[56], line 16\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m#ds = ds.map(lambda x, y: (x, tf.squeeze(y, axis=-1)))\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m#for window in ds:\u001B[39;00m\n\u001B[0;32m     13\u001B[0m      \u001B[38;5;66;03m#  print(list(window))\u001B[39;00m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ds\n\u001B[1;32m---> 16\u001B[0m train_set \u001B[38;5;241m=\u001B[39m \u001B[43mwindowed_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m test_set \u001B[38;5;241m=\u001B[39m windowed_dataset(test_data)\n\u001B[0;32m     19\u001B[0m t \u001B[38;5;241m=\u001B[39m train_set\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[1;32mIn[56], line 8\u001B[0m, in \u001B[0;36mwindowed_dataset\u001B[1;34m(series, window_size, batch_size, shuffle_buffer)\u001B[0m\n\u001B[0;32m      6\u001B[0m ds \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mshuffle(shuffle_buffer)\n\u001B[0;32m      7\u001B[0m ds \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m w: tf\u001B[38;5;241m.\u001B[39mcast(w, tf\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[1;32m----> 8\u001B[0m outAverage \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mConv1D(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m20\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m'\u001B[39m, strides\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)(\u001B[43mw\u001B[49m)\n\u001B[0;32m      9\u001B[0m ds \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m w: (w, outAverage))\n\u001B[0;32m     10\u001B[0m ds \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mbatch(batch_size)\u001B[38;5;66;03m#.prefetch(1)\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def windowed_dataset(series, window_size=GSettings.window_size, batch_size=GSettings.batch_size, shuffle_buffer=GSettings.SHUFFLE_BUFFER_SIZE):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window: window.batch(window_size))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: tf.cast(w, tf.float32))\n",
    "    #outAverage = tf.keras.layers.Conv1D(1, 20, padding='same', strides=1)(w)\n",
    "    ds = ds.map(lambda w: (w, outAverage))\n",
    "    ds = ds.batch(batch_size)#.prefetch(1)\n",
    "    #ds = ds.map(lambda x, y: (x, tf.squeeze(y, axis=-1)))\n",
    "    #for window in ds:\n",
    "     #  print(list(window))\n",
    "    return ds\n",
    "\n",
    "train_set = windowed_dataset(train_data)\n",
    "test_set = windowed_dataset(test_data)\n",
    "\n",
    "t = train_set.take(1)\n",
    "list(t.as_numpy_iterator())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:47:23.176570400Z",
     "start_time": "2023-10-30T20:47:23.060667200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import AutoEncoderModels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:48.589766800Z",
     "start_time": "2023-10-30T20:40:48.508606900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32, 32)\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 1024)              103424    \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 32, 32)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103424 (404.00 KB)\n",
      "Trainable params: 103424 (404.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = AutoEncoderModels.AlphabetMulti(GSettings.window_size) #AlphabetSimple()\n",
    "#autoencoder.build((GSettings.window_size))\n",
    "#plot_model(autoencoder.nne1, show_shapes=True, show_layer_names=True)\n",
    "#autoencoder.encoder1.summary()\n",
    "autoencoder.decoder1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:48.764275600Z",
     "start_time": "2023-10-30T20:40:48.589766800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# td = np.random.randint(0,2,GSettings.window_size)\n",
    "# td = np.reshape(td, (1, 540, 1))\n",
    "# print(td.shape)\n",
    "# autoencoder.encoder(td)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:48.844693300Z",
     "start_time": "2023-10-30T20:40:48.765276400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# td = np.zeros(54)\n",
    "# td[0] = 1\n",
    "# td = td[np.newaxis, :]\n",
    "# autoencoder.decoder(td)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:48.924029900Z",
     "start_time": "2023-10-30T20:40:48.845693100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "#autoencoder.nne1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:49.004646400Z",
     "start_time": "2023-10-30T20:40:48.925031Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class CustomMSE(tf.keras.losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "#        print(y_true.shape)\n",
    "#        cum_y_true = tf.cumsum(y_true)\n",
    "#        cum_y_pred = tf.cumsum(y_pred)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:49.087412600Z",
     "start_time": "2023-10-30T20:40:49.006645500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()#CustomMSE()\n",
    "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.BinaryAccuracy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:49.170906100Z",
     "start_time": "2023-10-30T20:40:49.088411500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def apply_gradient(optimizer, model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss_value = loss_object(y_true=y, y_pred=logits)# + loss_object2(y_true=y, y_pred=logits2)\n",
    "\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    return logits, loss_value\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:49.254239500Z",
     "start_time": "2023-10-30T20:40:49.173907300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def train_data_for_one_epoch(model, train, train_acc_metric, totalRecords):\n",
    "    losses = []\n",
    "    pbar = tqdm(total=totalRecords, position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ')\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_set):\n",
    "        logits, loss_value = apply_gradient(optimizer, model, x_batch_train, y_batch_train)\n",
    "\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        train_acc_metric(y_batch_train, logits)\n",
    "        pbar.set_description(\"Training loss for step %s: %.4f acc: %.4f\" % (int(step), float(loss_value), float(train_acc_metric.result())))\n",
    "        pbar.update()\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:40:49.334577900Z",
     "start_time": "2023-10-30T20:40:49.257240Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss for step 20: 0.6449 acc: 0.6141:   0%|          | 21/5737.96875 875 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStart of epoch \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (epoch,))\n\u001B[1;32m----> 7\u001B[0m     losses_train \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_data_for_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mautoencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_acc_metric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43mGSettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     train_acc \u001B[38;5;241m=\u001B[39m train_acc_metric\u001B[38;5;241m.\u001B[39mresult()\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m#losses_val = perform_validation()\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m#val_acc = val_acc_metric.result()\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[54], line 5\u001B[0m, in \u001B[0;36mtrain_data_for_one_epoch\u001B[1;34m(model, train, train_acc_metric, totalRecords)\u001B[0m\n\u001B[0;32m      3\u001B[0m pbar \u001B[38;5;241m=\u001B[39m tqdm(total\u001B[38;5;241m=\u001B[39mtotalRecords, position\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, bar_format\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{l_bar}\u001B[39;00m\u001B[38;5;132;01m{bar}\u001B[39;00m\u001B[38;5;124m| \u001B[39m\u001B[38;5;132;01m{n_fmt}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{total_fmt}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, (x_batch_train, y_batch_train) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_set):\n\u001B[1;32m----> 5\u001B[0m     logits, loss_value \u001B[38;5;241m=\u001B[39m \u001B[43mapply_gradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_batch_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss_value)\n\u001B[0;32m      9\u001B[0m     train_acc_metric(y_batch_train, logits)\n",
      "Cell \u001B[1;32mIn[53], line 6\u001B[0m, in \u001B[0;36mapply_gradient\u001B[1;34m(optimizer, model, x, y)\u001B[0m\n\u001B[0;32m      3\u001B[0m     logits \u001B[38;5;241m=\u001B[39m model(x)\n\u001B[0;32m      4\u001B[0m     loss_value \u001B[38;5;241m=\u001B[39m loss_object(y_true\u001B[38;5;241m=\u001B[39my, y_pred\u001B[38;5;241m=\u001B[39mlogits)\u001B[38;5;66;03m# + loss_object2(y_true=y, y_pred=logits2)\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m gradients \u001B[38;5;241m=\u001B[39m \u001B[43mtape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainable_weights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mapply_gradients(\u001B[38;5;28mzip\u001B[39m(gradients, model\u001B[38;5;241m.\u001B[39mtrainable_weights))\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logits, loss_value\n",
      "File \u001B[1;32m~\\.conda\\envs\\ae4\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1065\u001B[0m, in \u001B[0;36mGradientTape.gradient\u001B[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001B[0m\n\u001B[0;32m   1059\u001B[0m   output_gradients \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1060\u001B[0m       composite_tensor_gradient\u001B[38;5;241m.\u001B[39mget_flat_tensors_for_gradients(\n\u001B[0;32m   1061\u001B[0m           output_gradients))\n\u001B[0;32m   1062\u001B[0m   output_gradients \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(x)\n\u001B[0;32m   1063\u001B[0m                       \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m output_gradients]\n\u001B[1;32m-> 1065\u001B[0m flat_grad \u001B[38;5;241m=\u001B[39m \u001B[43mimperative_grad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimperative_grad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1066\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1067\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_targets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1068\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_sources\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1069\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_gradients\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_gradients\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1070\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources_raw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mflat_sources_raw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1071\u001B[0m \u001B[43m    \u001B[49m\u001B[43munconnected_gradients\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munconnected_gradients\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1073\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_persistent:\n\u001B[0;32m   1074\u001B[0m   \u001B[38;5;66;03m# Keep track of watched variables before setting tape to None\u001B[39;00m\n\u001B[0;32m   1075\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_watched_variables \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tape\u001B[38;5;241m.\u001B[39mwatched_variables()\n",
      "File \u001B[1;32m~\\.conda\\envs\\ae4\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001B[0m, in \u001B[0;36mimperative_grad\u001B[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[0;32m     64\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     65\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown value for unconnected_gradients: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m unconnected_gradients)\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_TapeGradient\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[0;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_gradients\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources_raw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_str\u001B[49m\u001B[43m(\u001B[49m\u001B[43munconnected_gradients\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ae4\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:117\u001B[0m, in \u001B[0;36m_gradient_function\u001B[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001B[0m\n\u001B[0;32m    108\u001B[0m   \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_control_flow_context\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[0;32m    110\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.GradientTape.gradients() does not support graph control flow \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    111\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moperations like tf.cond or tf.while at this time. Use tf.gradients() \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    112\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstead. If you need this feature, please file a feature request at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    113\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://github.com/tensorflow/tensorflow/issues/new\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    114\u001B[0m     )\n\u001B[1;32m--> 117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_gradient_function\u001B[39m(op_name, attr_tuple, num_inputs, inputs, outputs,\n\u001B[0;32m    118\u001B[0m                        out_grads, skip_input_indices, forward_pass_name_scope):\n\u001B[0;32m    119\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls the gradient function of the op.\u001B[39;00m\n\u001B[0;32m    120\u001B[0m \n\u001B[0;32m    121\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;124;03m    The gradients with respect to the inputs of the function, as a list.\u001B[39;00m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m    135\u001B[0m   mock_op \u001B[38;5;241m=\u001B[39m _MockOp(attr_tuple, inputs, outputs, op_name, skip_input_indices)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate over epochs.\n",
    "epochs = 2\n",
    "epochs_val_losses, epochs_train_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    losses_train = train_data_for_one_epoch(autoencoder, train_set, train_acc_metric, train_data.shape[0]/GSettings.batch_size)\n",
    "    train_acc = train_acc_metric.result()\n",
    "\n",
    "    #losses_val = perform_validation()\n",
    "    #val_acc = val_acc_metric.result()\n",
    "\n",
    "    losses_train_mean = np.mean(losses_train)\n",
    "    #losses_val_mean = np.mean(losses_val)\n",
    "    #epochs_val_losses.append(losses_val_mean)\n",
    "    epochs_train_losses.append(losses_train_mean)\n",
    "\n",
    "    print('\\n Epoch %s: Train loss: %.4f  Validation Loss: %.4f, Train Accuracy: %.4f, Validation Accuracy %.4f' % (epoch, float(losses_train_mean), 0#float(losses_val_mean)\n",
    "        , float(train_acc), 0#float(val_acc)\n",
    "        ))\n",
    "\n",
    "    train_acc_metric.reset_states()\n",
    "    #val_acc_metric.reset_states()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:47:14.016418300Z",
     "start_time": "2023-10-30T20:40:49.335578200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autoencoder.save(\"firstsuccess2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T10:45:51.842653100Z",
     "start_time": "2023-09-14T10:45:51.842653100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import sys\n",
    "class CustomMSE(tf.keras.losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        print(y_true.shape)\n",
    "        #mse1 = tf.reduce_mean(tf.square(y_true[:, 0] - y_pred[:, 0]))\n",
    "        #mse2 = tf.reduce_mean(tf.square(y_true[:, 1] - y_pred[:, 1]))\n",
    "        #total_mse = mse1 + mse2\n",
    "        #return total_mse\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def custom_loss_function(y_true, y_pred):\n",
    "    tf.print(\"\\n y_true\", y_true, output_stream=sys.stdout)\n",
    "    tf.print(\"\\n y_pred\", y_pred, output_stream=sys.stdout)\n",
    "\n",
    "   # y_true = tf.cast(y_true, tf.float32)\n",
    "    #y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "\n",
    "    #squared_difference = tf.square(y_true - y_pred)\n",
    "    #return tf.reduce_mean(squared_difference)\n",
    "    return tf.square(y_pred)\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy']) #'binary_crossentropy'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T10:37:42.045183300Z",
     "start_time": "2023-09-08T10:37:41.842173500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#from tensorflow.keras.losses import mean_squared_error\n",
    "autoencoder.compile(loss = 'binary_crossentropy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T10:37:42.235233Z",
     "start_time": "2023-09-08T10:37:42.046183500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5722/5722 [==============================] - 296s 52ms/step - loss: 0.7989\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(train_set, epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T10:42:38.880303600Z",
     "start_time": "2023-09-08T10:37:42.236235400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "0.865458"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 1, 0, 0]\n",
    "y_pred = [-18.6, 0.51, 2.94, -12.8]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "bce(y_true, y_pred).numpy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T10:42:39.081630Z",
     "start_time": "2023-09-08T10:42:38.880303600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.6666667>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[.0, 1.0, 0]]\n",
    "y_pred = [[.0, 0.8, 0.6]]\n",
    "   \n",
    "la = tf.keras.metrics.BinaryAccuracy()\n",
    "la.reset_state()\n",
    "la.update_state(y_true, y_pred)\n",
    "la.result()\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T23:48:06.232411100Z",
     "start_time": "2023-09-09T23:48:06.150784300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
